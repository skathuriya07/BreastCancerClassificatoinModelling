{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Classification
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group number: 49  , SID1: 480291367 , SID2: 480511335  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Breast Cancer Wisconsin Dataset\n",
    "\n",
    "# All missing value '?' will be transformed to na_values \n",
    "df = pd.read_csv('breast-cancer-wisconsin.csv', na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "# rebuilding class from string {'class 1', 'class 2'} \n",
    "# to Integer {0, 1} \n",
    "mapping = {'class1': 0, 'class2': 1}\n",
    "df1 = df.replace({'class': mapping})\n",
    "df2 = df.replace({'class': mapping})\n",
    "\n",
    "# replacing all the missing value in the dataframe\n",
    "# df2 with the mean value of the column \n",
    "imputer = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "df2[['Clump Thickness', \n",
    "     'Uniformity of Cell Size', \n",
    "     'Uniformity of Cell Shape', \n",
    "     'Marginal Adhesion', \n",
    "     ' Single Epithelial Cell Size', \n",
    "     'Bare Nuclei', \n",
    "     'Bland Chromatin', \n",
    "     'Normal Nucleoli', \n",
    "     'Mitoses']] = imputer.fit_transform(\n",
    "                       df2[['Clump Thickness', \n",
    "                      'Uniformity of Cell Size', \n",
    "                      'Uniformity of Cell Shape',\n",
    "                      'Marginal Adhesion', \n",
    "                      ' Single Epithelial Cell Size', \n",
    "                      'Bare Nuclei', \n",
    "                      'Bland Chromatin', \n",
    "                      'Normal Nucleoli', \n",
    "                      'Mitoses']])\n",
    "\n",
    "# replacing all the missing value in the dataframe \n",
    "# with the mean value of the column \n",
    "# and convert the dataframe to np-array df1\n",
    "imputer = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "df1 = imputer.fit_transform(df1)\n",
    "\n",
    "# Pre-process dataset - min-max Normalisation \n",
    "\n",
    "# Normalise each attributes in the dataframe df2 using a min-max scaler \n",
    "# to the values between [0,1] \n",
    "scaler = MinMaxScaler()\n",
    "df2[['Clump Thickness', \n",
    "     'Uniformity of Cell Size', \n",
    "     'Uniformity of Cell Shape', \n",
    "     'Marginal Adhesion', \n",
    "     ' Single Epithelial Cell Size', \n",
    "     'Bare Nuclei', \n",
    "     'Bland Chromatin', \n",
    "     'Normal Nucleoli', \n",
    "     'Mitoses']] = scaler.fit_transform(\n",
    "                        df2[['Clump Thickness', \n",
    "                         'Uniformity of Cell Size', \n",
    "                         'Uniformity of Cell Shape',\n",
    "                         'Marginal Adhesion', \n",
    "                         ' Single Epithelial Cell Size', \n",
    "                         'Bare Nuclei', \n",
    "                         'Bland Chromatin', \n",
    "                         'Normal Nucleoli', \n",
    "                         'Mitoses']])\n",
    "     \n",
    "# Normalise each attributes in the np-array df1 \n",
    "# using a min-max scaler to the values between [0,1]\n",
    "df1 = scaler.fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444, 0.0000, 0.0000, 0.0000, 0.1111, 0.0000, 0.2222, 0.0000, 0.0000, 0\n",
      "0.4444, 0.3333, 0.3333, 0.4444, 0.6667, 1.0000, 0.2222, 0.1111, 0.0000, 0\n",
      "0.2222, 0.0000, 0.0000, 0.0000, 0.1111, 0.1111, 0.2222, 0.0000, 0.0000, 0\n",
      "0.5556, 0.7778, 0.7778, 0.0000, 0.2222, 0.3333, 0.2222, 0.6667, 0.0000, 0\n",
      "0.3333, 0.0000, 0.0000, 0.2222, 0.1111, 0.0000, 0.2222, 0.0000, 0.0000, 0\n",
      "0.7778, 1.0000, 1.0000, 0.7778, 0.6667, 1.0000, 0.8889, 0.6667, 0.0000, 1\n",
      "0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 1.0000, 0.2222, 0.0000, 0.0000, 0\n",
      "0.1111, 0.0000, 0.1111, 0.0000, 0.1111, 0.0000, 0.2222, 0.0000, 0.0000, 0\n",
      "0.1111, 0.0000, 0.0000, 0.0000, 0.1111, 0.0000, 0.0000, 0.0000, 0.4444, 0\n",
      "0.3333, 0.1111, 0.0000, 0.0000, 0.1111, 0.0000, 0.1111, 0.0000, 0.0000, 0\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places\n",
    "display_list = []\n",
    "\n",
    "# Using a while loop to extract all the first-four-line attributes\n",
    "i = 0\n",
    "while i < 10:\n",
    "    tmp_list = []\n",
    "    for n in df1[i]:\n",
    "        tmp_list.append(n)\n",
    "    display_list.append(tmp_list)\n",
    "    i+=1 \n",
    "    \n",
    "# Reading all the attributes from the display_list and \n",
    "# using self-designed printing method print out all\n",
    "# the attributes\n",
    "return_count = 0\n",
    "for row in display_list:\n",
    "    for n in row:\n",
    "        return_count +=1\n",
    "        if return_count == 10:\n",
    "            print(int(n))\n",
    "            return_count = 0\n",
    "        else: \n",
    "            print(\"{:.4f}\".format(n), end = \", \")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be \n",
    "# provided to the classifiers\n",
    "\n",
    "# x and y\n",
    "x = df1[:, :-1] # for all but last column\n",
    "y = df1[:, -1] # for last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassifier(X, y, k):\n",
    "  \"\"\"\n",
    "  This function is for K-neaest neighbour classifier. \n",
    "  It takes input of data set, output class and number of \n",
    "  neighbours, fits the data in classifier, gives prediction \n",
    "  for the calss and calculates mean score for \n",
    "  cross validation folds.\n",
    "  Parameters:\n",
    "  X: dataFrame: data columns except the prediction class\n",
    "  y: dataFrame: prediction class\n",
    "  k: int: number of nearest neighbours\n",
    "  Return:\n",
    "  scores.mean(): Float : the mean of scores calculated \n",
    "  for 10 fold stratified cross-validation\n",
    "  \"\"\"\n",
    "  knn = KNeighborsClassifier(n_neighbors = k)\n",
    "  knn.fit(X, y)\n",
    "  scores = cross_val_score(knn, X, y, cv=cvKFold)\n",
    "  return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):  \n",
    "  \"\"\"\n",
    "  This function is for Logistic Regression classifier. \n",
    "  It takes input of data set and output class, fits the data \n",
    "  in classifier and calculates mean score for cross \n",
    "  validation folds.\n",
    "  Parameters:\n",
    "  X: dataFrame: data columns except the prediction class\n",
    "  y: dataFrame: prediction class\n",
    "  Return:\n",
    "  scores.mean(): Float : the mean of scores calculated \n",
    "  for 10 fold stratified cross-validation\n",
    "  \"\"\"\n",
    "  logreg = LogisticRegression()\n",
    "  logreg.fit(X, y)\n",
    "  scores = cross_val_score(logreg, X, y, cv=cvKFold)\n",
    "  return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "  \"\"\"\n",
    "  This function is for Naïve Bayes classifier. It takes \n",
    "  input of data set and output class, fits the data in classifier a\n",
    "  nd calculates mean score for cross validation folds.\n",
    "  Parameters:\n",
    "  X: dataFrame: data columns except the prediction class\n",
    "  y: dataFrame: prediction class\n",
    "  Return:\n",
    "  scores.mean(): Float : the mean of scores calculated \n",
    "  for 10 fold stratified cross-validation\n",
    "  \"\"\"\n",
    "  nb = GaussianNB()\n",
    "  nb.fit(X, y)\n",
    "  scores = cross_val_score(nb, X, y, cv=cvKFold)\n",
    "  return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "  \"\"\"\n",
    "  This function is for Decision Tree classifier. It takes \n",
    "  input of data set and output class, fits the data in \n",
    "  classifier and calculates mean score for cross validation folds.\n",
    "  Parameters:\n",
    "  X: dataFrame: data columns except the prediction class\n",
    "  y: dataFrame: prediction class\n",
    "  Return:\n",
    "  scores.mean(): Float : the mean of scores calculated \n",
    "  for 10 fold stratified cross-validation\n",
    "  \"\"\"\n",
    "  tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "  tree.fit(X, y)\n",
    "  scores = cross_val_score(tree, X, y, cv=cvKFold)\n",
    "  return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    # setting bagging \n",
    "    bag_clf = BaggingClassifier(\n",
    "      DecisionTreeClassifier(max_depth=max_depth, \n",
    "                             criterion='entropy'), \n",
    "      n_estimators = n_estimators, \n",
    "                    max_samples=max_samples, \n",
    "      bootstrap=True, random_state=0)\n",
    "    bag_clf.fit(X, y)\n",
    "    scores = cross_val_score(bag_clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "# Fucntion adaDTClassifier takes five inputs: X (np-array)- \n",
    "# attribute values, y (np-array)- the class, n_estimators \n",
    "# (Integer), learning_rate (float), and max_depth (Integer)\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    # setting AdaBoost\n",
    "    ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(criterion='entropy', \n",
    "                           max_depth=max_depth), \n",
    "    n_estimators=n_estimators, \n",
    "                learning_rate=learning_rate, \n",
    "                random_state=0) \n",
    "    ada_clf.fit(X, y)\n",
    "    scores = cross_val_score(ada_clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "# Fucntion gbClassifier takes four inputs: X \n",
    "# (np-array)- attribute values, y (np-array)- the class, \n",
    "# n_estimators (Integer), and learning_rate (float) \n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    #setting Gradient Boosting\n",
    "    gb_clf = GradientBoostingClassifier(max_depth=1, \n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate, \n",
    "                random_state=0)\n",
    "    gb_clf.fit(X, y)\n",
    "    scores = cross_val_score(gb_clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN average cross-validation accuracy: 0.9642\n",
      "LR average cross-validation accuracy:0.9642\n",
      "NB average cross-validation accuracy: 0.9585\n",
      "DT average cross-validation accuracy: 0.9385\n",
      "Bagging average cross-validation accuracy: 0.9557\n",
      "AdaBoost average cross-validation accuracy: 0.9599\n",
      "GB average cross-validation accuracy: 0.9571\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "#KNN\n",
    "k=3\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in \n",
    "# part 1 to 4 decimal places here:\n",
    "print(\"kNN average cross-validation accuracy: %.4f\" \n",
    "      %kNNClassifier(x, y, k))\n",
    "print(\"LR average cross-validation accuracy:%.4f\" \n",
    "      %logregClassifier(x, y))\n",
    "print(\"NB average cross-validation accuracy: %.4f\" \n",
    "      %nbClassifier(x, y))\n",
    "print(\"DT average cross-validation accuracy: %.4f\" \n",
    "      %dtClassifier(x, y))\n",
    "print(\"Bagging average cross-validation accuracy: %.4f\" \n",
    "      %bagDTClassifier(x, y, bag_n_estimators, \n",
    "                bag_max_samples, bag_max_depth))\n",
    "print(\"AdaBoost average cross-validation accuracy: %.4f\" \n",
    "      %adaDTClassifier(x, y, ada_n_estimators, \n",
    "            ada_learning_rate, ada_bag_max_depth))\n",
    "print(\"GB average cross-validation accuracy: %.4f\" \n",
    "      %gbClassifier(x, y, gb_n_estimators, gb_learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'linear'\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100] \n",
    "gamma = [0.001, 0.01, 0.1, 1, 10, 100] \n",
    "def bestLinClassifier(X,y):\n",
    "  \"\"\"\n",
    "  This function is for calculating  the best C and \n",
    "  gamma value, CV accuracy and test set accuracy. \n",
    "  It takes input of data set and output class, splits \n",
    "  it to testing and training data and fits  SVC model\n",
    "  linear kernel. Then it performs grid search with \n",
    "  10 fold cross validation. Upon doing that, it checks \n",
    "  and gives the best parameter (C and gamma values), \n",
    "  CV accuracy and test set accuracy\n",
    "  Parameters:\n",
    "  X: dataFrame: data columns except the prediction class\n",
    "  y: dataFrame: prediction class\n",
    "  Return:\n",
    "  return_values: list: consist of a dicitonary with best \n",
    "  paramater values (C and gamma), CV accuracy and test\n",
    "  set accuracay\n",
    "  \"\"\"\n",
    "  param_grid = {'C': C, 'gamma': gamma}\n",
    "  \n",
    "  return_values = []\n",
    "  # splitting to train and test\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                    y, random_state=0)\n",
    "  # Fitting linear SVC model\n",
    "  lin_svm = SVC(kernel=\"linear\")\n",
    "  lin_svm_fit = lin_svm.fit(X_train, y_train)\n",
    "  # Finding the best C and gamma values using GridSearch\n",
    "  # Performing GridSearchCV on training set\n",
    "  grid_search = GridSearchCV(lin_svm, param_grid, \n",
    "                cv=10, return_train_score=True)\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  # Accuracy of fit\n",
    "  # accuracy = grid_search.score(X_test, y_test)\n",
    "  return_values.append(grid_search.best_params_)\n",
    "  # CV accuracy\n",
    "  cv_acccuracy = (grid_search.best_score_)\n",
    "  return_values.append(cv_acccuracy)\n",
    "  # test set accuracy\n",
    "  y_pred = lin_svm.predict(X_test)\n",
    "  test_accuracy = accuracy_score(y_test, y_pred)\n",
    "  return_values.append(test_accuracy)\n",
    "  return return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from \n",
    "# sklearn.ensemble with information gain and \n",
    "# max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 20, 30, 50, 100]\n",
    "max_leaf_nodes = [4, 10, 16, 20, 30]\n",
    "\n",
    "def bestRFClassifier(X,y):\n",
    "    \"\"\"\n",
    "    This function is for calculating  the best n_estimators \n",
    "    and max_leaf_nodes value, CV accuracy and test set accuracy. \n",
    "    It takes input of data set and output class, splits it to \n",
    "    testing and training data and fits  RandomForest model. \n",
    "    Then it performs grid search with 10 fold cross validation. \n",
    "    Upon doing that, it checks and gives the best parameter \n",
    "    (n_estimators and max_leaf_nodes), CV accuracy and test \n",
    "    set accuracy\n",
    "    Parameters:\n",
    "    X: dataFrame: data columns except the prediction class\n",
    "    y: dataFrame: prediction class\n",
    "    Return:\n",
    "    return_values: list: consist of a dicitonary with best \n",
    "    paramater values (C and gamma), CV accuracy and test\n",
    "    set accuracay\n",
    "    \"\"\" \n",
    "  \n",
    "    param_grid = {'n_estimators': n_estimators, \n",
    "                  'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "    # splitting to train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                    y, random_state=0)\n",
    "\n",
    "    #Initializing list Best_score - \n",
    "    # [test_score, {n_estimators, max_leaf_nodes}, best_score]\n",
    "    Best_score = []\n",
    "\n",
    "    #setting up Random Forest\n",
    "    rf = RandomForestClassifier(criterion='entropy', \n",
    "                        max_features=\"sqrt\", random_state=0)\n",
    "\n",
    "    # performing grid search \n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=10, \n",
    "                            return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Extracting needed data by using built-in function\n",
    "    Best_score.append(grid_search.score(X_test, y_test))\n",
    "    Best_score.append(grid_search.best_params_)\n",
    "    Best_score.append(grid_search.best_score_)\n",
    "\n",
    "    return Best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best C: 10.0000\n",
      "SVM best gamma: 0.0010\n",
      "SVM cross-validation accuracy: 0.9677\n",
      "SVM test set accuracy: 0.9600\n",
      "RF best n_estimators: 100\n",
      "RF best max_leaf_nodes: 16\n",
      "RF cross-validation accuracy: 0.9696\n",
      "RF test set accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold Stratified \n",
    "# Cross Validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should \n",
    "# be provided to GridSearchV\n",
    "# This should include using train_test_split \n",
    "# from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. \n",
    "# All results should be printed to 4 decimal places except for\n",
    "# \"n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "lin_classifier_result = bestLinClassifier(x,y)\n",
    "print(\"SVM best C: {:.4f}\"\n",
    "      .format(lin_classifier_result[0]['C']))\n",
    "print(\"SVM best gamma: {:.4f}\"\n",
    "      .format(lin_classifier_result[0]['gamma']))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\"\n",
    "      .format(lin_classifier_result[1]))\n",
    "print(\"SVM test set accuracy: {:.4f}\"\n",
    "      .format(lin_classifier_result[2]))\n",
    "\n",
    "RF_result = bestRFClassifier(x, y)\n",
    "print(\"RF best n_estimators: {}\"\n",
    "      .format(RF_result[1]['n_estimators'])) \n",
    "print(\"RF best max_leaf_nodes: {}\"\n",
    "      .format(RF_result[1]['max_leaf_nodes']))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\"\n",
    "      .format(RF_result[2]))\n",
    "print(\"RF test set accuracy: {:.4f}\"\n",
    "      .format(RF_result[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
